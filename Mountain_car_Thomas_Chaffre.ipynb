{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chaffre/Bio-Inspired-ER/blob/main/Mountain_car_Thomas_Chaffre.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAztKybybMv_",
        "outputId": "42fc01c7-c8c3-4df3-c658-0b73b9579dc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Collecting torch==1.4.0+cu100\n",
            "  Downloading https://download.pytorch.org/whl/cu100/torch-1.4.0%2Bcu100-cp38-cp38-linux_x86_64.whl (723.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m723.9/723.9 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.5.0+cu100\n",
            "  Downloading https://download.pytorch.org/whl/cu100/torchvision-0.5.0%2Bcu100-cp38-cp38-linux_x86_64.whl (4.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.5.0+cu100) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from torchvision==0.5.0+cu100) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision==0.5.0+cu100) (1.21.6)\n",
            "Installing collected packages: torch, torchvision\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.13.1+cu116\n",
            "    Uninstalling torch-1.13.1+cu116:\n",
            "      Successfully uninstalled torch-1.13.1+cu116\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.14.1+cu116\n",
            "    Uninstalling torchvision-0.14.1+cu116:\n",
            "      Successfully uninstalled torchvision-0.14.1+cu116\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.14.1 requires torch==1.13.1, but you have torch 1.4.0+cu100 which is incompatible.\n",
            "torchaudio 0.13.1+cu116 requires torch==1.13.1, but you have torch 1.4.0+cu100 which is incompatible.\n",
            "fastai 2.7.10 requires torch<1.14,>=1.7, but you have torch 1.4.0+cu100 which is incompatible.\n",
            "fastai 2.7.10 requires torchvision>=0.8.2, but you have torchvision 0.5.0+cu100 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-1.4.0+cu100 torchvision-0.5.0+cu100\n"
          ]
        }
      ],
      "source": [
        "!pip3 install torch==1.4.0+cu100 torchvision==0.5.0+cu100 -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsSFD5tt7IrR",
        "outputId": "ab191f30-e578-46f5-c9cf-6ce6f43b6426"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gym[classic_control] in /usr/local/lib/python3.8/dist-packages (0.25.2)\n",
            "Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (6.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (2.2.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (1.21.6)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.8/dist-packages (from gym[classic_control]) (0.0.8)\n",
            "Collecting pygame==2.1.0\n",
            "  Downloading pygame-2.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata>=4.8.0->gym[classic_control]) (3.11.0)\n",
            "Installing collected packages: pygame\n",
            "Successfully installed pygame-2.1.0\n"
          ]
        }
      ],
      "source": [
        "pip install gym[classic_control]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTRxOSiq8Ejw",
        "outputId": "d767b09a-e107-4443-f2ad-e19f3d2e9ffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pygame 2.1.0 (SDL 2.0.16, Python 3.8.10)\n",
            "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Surface(640x480x32 SW)>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import os\n",
        "os.environ['SDL_VIDEODRIVER']='dummy'\n",
        "import pygame\n",
        "pygame.display.set_mode((640,480))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LF2VwNflb_UH",
        "outputId": "df035b5c-8538-429c-a8eb-d905ea67ff58"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device used: cpu\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "import time\n",
        "import matplotlib.pyplot as plt \n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as f\n",
        "\n",
        "# Use Cuda GPU, if not available CPU\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device used: \" + str(device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lZ52xAWJcr4O"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "\n",
        "    def __init__(self,state_dim,act_dim):\n",
        "        super(Critic,self).__init__()\n",
        "        \n",
        "        self.fc1=nn.Linear(11,16)\n",
        "        self.fc2=nn.Linear(16,16)\n",
        "        self.fc3=nn.Linear(16,1)\n",
        "\n",
        "        self.fc1.weight.data.normal_(mean=0.0, std=1.0/(np.sqrt(0.5*(11))))\n",
        "        self.fc1.bias.data.fill_(0.1)\n",
        "        \n",
        "        self.fc2.weight.data.normal_(mean=0.0, std=1.0/(np.sqrt(0.5*16)))\n",
        "        self.fc2.bias.data.fill_(0.1)\n",
        "        \n",
        "        self.fc3.weight.data.normal_(mean=0.0, std=1.0/(np.sqrt(0.5*16)))\n",
        "        self.fc3.bias.data.fill_(0.1)\n",
        "        \n",
        "        self.Ln_1 = nn.LayerNorm(16)\n",
        "        self.Ln_2 = nn.LayerNorm(16)\n",
        "        \n",
        "    def forward(self,state,action):\n",
        "        \n",
        "        state = torch.reshape(state, (1, 10))\n",
        "        action = torch.reshape(action, (1, 1))\n",
        "\n",
        "        x = torch.cat([state, action], 1)\n",
        "        \n",
        "        x = f.leaky_relu(self.Ln_1(self.fc1(x)))\n",
        "\n",
        "        x = f.leaky_relu(self.Ln_2(self.fc2(x)))\n",
        "\n",
        "        x = self.fc3(x)\n",
        "        \n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "LdGuiuNlc0dN"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "\n",
        "    def __init__(self,state_dim,act_dim):\n",
        "        super(Actor,self).__init__()\n",
        "        \n",
        "        self.fc1=nn.Linear(10,16)\n",
        "        self.fc2=nn.Linear(16,16)\n",
        "        self.fc4=nn.Linear(16,16)\n",
        "        \n",
        "        self.fc3=nn.Linear(16,3)\n",
        "\n",
        "        self.fc1.weight.data.normal_(mean=0.0, std=1.0/(np.sqrt(0.5*(10))))\n",
        "        self.fc1.bias.data.fill_(0.1)\n",
        "        \n",
        "        self.fc2.weight.data.normal_(mean=0.0, std=1.0/(np.sqrt(0.5*16)))\n",
        "        self.fc2.bias.data.fill_(0.1)\n",
        "\n",
        "        self.fc4.weight.data.normal_(mean=0.0, std=1.0/(np.sqrt(0.5*16)))\n",
        "        self.fc4.bias.data.fill_(0.1)\n",
        "        \n",
        "        self.fc3.weight.data.normal_(mean=0.0, std=1.0/(np.sqrt(0.5*16)))\n",
        "        self.fc3.bias.data.fill_(0.1)\n",
        "        \n",
        "        self.Ln_1 = nn.LayerNorm(16)\n",
        "        self.Ln_2 = nn.LayerNorm(16)\n",
        "        self.Ln_4 = nn.LayerNorm(16)\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "\n",
        "      x = f.leaky_relu(self.Ln_1(self.fc1(x)))\n",
        "        \n",
        "      x = f.leaky_relu(self.Ln_2(self.fc2(x)))\n",
        "\n",
        "      #x = f.relu(self.Ln_4(self.fc4(x)))\n",
        "\n",
        "      x = self.fc3(x)\n",
        "        \n",
        "      return torch.tanh(x)\n",
        "\n",
        "    def noise(self, noise_std):\n",
        "      \n",
        "      self.fc1.weight.add_(torch.randn(2).to(device) * noise_std)\n",
        "      self.fc1.bias.add_(torch.randn(16).to(device) * noise_std)\n",
        "      \n",
        "      self.fc2.weight.add_(torch.randn(16).to(device) * noise_std)\n",
        "      self.fc2.bias.add_(torch.randn(16).to(device) * noise_std)\n",
        "      \n",
        "      self.fc3.weight.add_(torch.randn(16).to(device) * noise_std)\n",
        "      self.fc3.bias.add_(torch.randn(3).to(device) * noise_std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8f6qe3D3c797"
      },
      "outputs": [],
      "source": [
        "class Bio_inspired_Buffer:\n",
        "   \n",
        "   def __init__(self, capacity):\n",
        "      self.capacity = capacity\n",
        "      self.buffers = [[], []]\n",
        "      self.position = [0, 0]\n",
        "      \n",
        "   def push(self, state, action, reward, next_state, done, buffer_nb):\n",
        "      \n",
        "      if len(self.buffers[buffer_nb]) < self.capacity[buffer_nb]:\n",
        "         self.buffers[buffer_nb].append(None)\n",
        "\n",
        "      self.buffers[buffer_nb][self.position[buffer_nb]] = (state, action, reward, next_state, done)\n",
        "      self.position[buffer_nb] = (self.position[buffer_nb] + 1) % self.capacity[buffer_nb]\n",
        "   \n",
        "   def CER(self, batch_size):\n",
        "      \n",
        "      batch = random.sample(self.buffers[0], batch_size)\n",
        "      \n",
        "      #batch.append(self.buffers[0][-1]) # CER Technique, source : https://arxiv.org/abs/1712.01275\n",
        "\n",
        "      state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
        "      \n",
        "      return state, action, reward, next_state, done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "BCtvLIZydICP"
      },
      "outputs": [],
      "source": [
        "def soft_q_update(batch_size, update_ratio, update,\n",
        "              gamma = 0.99,\n",
        "              mean_lambda = 1e-3,\n",
        "              std_lambda = 1e-3,\n",
        "              z_lambda = 0.0,\n",
        "              soft_tau = 5e-3,\n",
        "              total_q_value_loss = [],\n",
        "              total_q_value_loss_2 = [],\n",
        "              total_value_loss = [],\n",
        "              total_policy_loss = [],\n",
        "              total_alpha_loss = [],\n",
        "              ):\n",
        "\n",
        "   state, action, reward, next_state, done = replay_buffer.CER(batch_size)\n",
        "   '''\n",
        "   state = [x+np.random.normal(0.0, 0.2) for x in state]\n",
        "   next_state = [x+np.random.normal(0.0, 0.2) for x in next_state]\n",
        "   action = [x+np.random.normal(0.0, 0.2) for x in action]\n",
        "   '''\n",
        "   state      = torch.FloatTensor(state).to(device)\n",
        "   next_state = torch.FloatTensor(next_state).to(device)\n",
        "   action     = torch.FloatTensor(action).to(device)\n",
        "   reward     = torch.FloatTensor(reward).unsqueeze(1).to(device)\n",
        "   done       = torch.FloatTensor(np.float32(done)).unsqueeze(1).to(device)\n",
        "   \n",
        "   expected_q_value = soft_q_net(state, action)\n",
        "   expected_q_value_2 = soft_q_net_2(state, action)\n",
        "\n",
        "   #new_action = policy_net.forward(next_state)\n",
        "\n",
        "   new_action = policy_net.forward(next_state)\n",
        "\n",
        "   for i in range(len(new_action)):\n",
        "     \n",
        "     #if np.random.uniform(0.0, 1.0) < 0.50:\n",
        "\n",
        "       #new_action[i] = np.random.randint(3)\n",
        "     \n",
        "     #else:\n",
        "       \n",
        "     new_action[i] = torch.nn.functional.softmax(new_action[i]) #+np.random.normal(0.0, 0.1))\n",
        "\n",
        "   n_a = []\n",
        "   for i in range(len(new_action)):\n",
        "      \n",
        "      n_a.append((torch.argmax(new_action[i],dim=0).tolist()))\n",
        "   \n",
        "   n_a = torch.FloatTensor(n_a).to(device)\n",
        "\n",
        "   target_value = torch.min(target_q_net(next_state, n_a), target_q_net_2(next_state, n_a))\n",
        "   next_q_value = reward + (1 - done) * gamma * target_value\n",
        "   \n",
        "   q_value_loss = criterion(expected_q_value, next_q_value.detach())\n",
        "\n",
        "   if update_ratio % 1000 == 0:\n",
        "\n",
        "     print(\"Value error = \" + str(abs(next_q_value[0].detach() - expected_q_value[0])))\n",
        "   \n",
        "   q_value_loss_2 = criterion(expected_q_value_2, next_q_value.detach())\n",
        "\n",
        "   soft_q_optimizer.zero_grad()\n",
        "   q_value_loss.backward()\n",
        "   #nn.utils.clip_grad_norm_(soft_q_net.parameters(), max_norm=1.0, norm_type=2)\n",
        "   soft_q_optimizer.step()\n",
        "   \n",
        "   soft_q_optimizer_2.zero_grad()\n",
        "   q_value_loss_2.backward()\n",
        "   #nn.utils.clip_grad_norm_(soft_q_net_2.parameters(), max_norm=1.0, norm_type=2)\n",
        "   soft_q_optimizer_2.step()\n",
        "   \n",
        "   #new_action = policy_net.forward(state)\n",
        "\n",
        "   if update_ratio % 2 == 0:\n",
        "     \n",
        "     new_action = policy_net.forward(state)\n",
        "      \n",
        "     for i in range(len(new_action)):\n",
        "         \n",
        "        new_action[i] = torch.nn.functional.softmax(new_action[i])\n",
        "      \n",
        "     n_a = []\n",
        "     for i in range(len(new_action)):\n",
        "\n",
        "      #if np.random.uniform(0.0, 1.0) < 0.10:\n",
        "\n",
        "        #n_a.append(np.random.randint(3))\n",
        "      \n",
        "      #else:\n",
        "        \n",
        "       n_a.append(torch.argmax(new_action[i],dim=0).tolist())\n",
        "\n",
        "     n_a = torch.FloatTensor(n_a).to(device)\n",
        "\n",
        "     expected_new_q_value = soft_q_net(state, n_a)  \n",
        "     expected_new_q_value_2 = soft_q_net_2(state, n_a)\n",
        "      \n",
        "     log_prob_target = torch.min(expected_new_q_value, expected_new_q_value_2)\n",
        "\n",
        "     policy_loss = -log_prob_target\n",
        "\n",
        "     policy_loss = policy_loss.mean()\n",
        "\n",
        "     policy_optimizer.zero_grad()\n",
        "     policy_loss.sum().backward()\n",
        "     #nn.utils.clip_grad_norm_(policy_net.parameters(), max_norm=1.0, norm_type=2)\n",
        "     policy_optimizer.step()\n",
        "\n",
        "     #if update_ratio % 1000 == 0:\n",
        "     #print(\"TARGET UPDATE\")\n",
        "     soft_tau = 5e-3\n",
        "     for target_param, param in zip(target_q_net.parameters(), soft_q_net.parameters()):\n",
        "       target_param.data.copy_(target_param.data * (1.0 - soft_tau) + param.data * soft_tau)\n",
        "\n",
        "     for target_param, param in zip(target_q_net_2.parameters(), soft_q_net_2.parameters()):\n",
        "       target_param.data.copy_(target_param.data * (1.0 - soft_tau) + param.data * soft_tau)\n",
        "\n",
        "   #Store losses for plot\n",
        "   #if update_ratio % 200 == 0:\n",
        "   #  total_q_value_loss.append(q_value_loss.item())\n",
        "   #  total_q_value_loss_2.append(q_value_loss_2.item())\n",
        "   #  total_policy_loss.append(policy_loss.sum())\n",
        "   \n",
        "    # PRINT LOSSES\n",
        "   if (update_ratio % 100 == 0 and update == True):\n",
        "     print('Iter :- ', update_ratio,\n",
        "            ' Q_value Loss :- ', round(q_value_loss.data.item(), 2),\n",
        "            ' Q_value Loss_2 :- ', round(q_value_loss_2.data.item(), 2),\n",
        "            #' Value Loss :- ', round(value_loss.data.item(), 2),\n",
        "            ' Policy Loss :- ', round(float(policy_loss.sum()),2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oItS1KKIdNQa",
        "outputId": "4e286bfa-9248-4f99-d9ed-6b4cd871c72a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The observation space: Box([-1.2  -0.07], [0.6  0.07], (2,), float32)\n",
            "The action space: Discrete(3)\n"
          ]
        }
      ],
      "source": [
        "# Initialize buffer with size of 1.10^6\n",
        "replay_buffer_size = [100000, 10000]\n",
        "replay_buffer = Bio_inspired_Buffer(replay_buffer_size)\n",
        "\n",
        "soft_q_net = Critic(2, 3).to(device)\n",
        "target_q_net = Critic(2, 3).to(device)\n",
        "\n",
        "soft_q_net_2 = Critic(2, 3).to(device)\n",
        "target_q_net_2 = Critic(2, 3).to(device)\n",
        "\n",
        "policy_net = Actor(2, 3).to(device)\n",
        "\n",
        "policy_net_perturbed = Actor(2, 3).to(device)\n",
        "\n",
        "for target_param, param in zip(policy_net_perturbed.parameters(), policy_net.parameters()):\n",
        "      target_param.data.copy_(param.data)\n",
        "\n",
        "for target_param, param in zip(target_q_net.parameters(), soft_q_net.parameters()):\n",
        "      target_param.data.copy_(param.data)\n",
        "\n",
        "for target_param, param in zip(target_q_net_2.parameters(), soft_q_net_2.parameters()):\n",
        "   target_param.data.copy_(param.data)\n",
        "\n",
        "criterion = nn.SmoothL1Loss()\n",
        "\n",
        "soft_q_optimizer = optim.Adam(soft_q_net.parameters(), lr=3e-4)#, weight_decay=wd)\n",
        "soft_q_optimizer_2 = optim.Adam(soft_q_net_2.parameters(), lr=3e-4)#, weight_decay=wd)\n",
        "policy_optimizer = optim.Adam(policy_net.parameters(), lr=3e-4) #, weight_decay=wd)\n",
        "\n",
        "env = gym.make('MountainCar-v0')\n",
        "\n",
        "obs_space = env.observation_space\n",
        "action_space = env.action_space\n",
        "print(\"The observation space: {}\".format(obs_space))\n",
        "print(\"The action space: {}\".format(action_space))\n",
        "\n",
        "# Number of steps you run the agent for \n",
        "num_steps = 200\n",
        "update_ratio = 0\n",
        "\n",
        "noise_std = 2.0\n",
        "\n",
        "epsilon = 1.0\n",
        "\n",
        "nb_step = 0\n",
        "\n",
        "cpt = False\n",
        "\n",
        "reward_ep = -200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bJzcWPwHdRT4",
        "outputId": "6d62befe-4222-4af7-cc92-b62adc485ba7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode : 0\n",
            "//////\n",
            "Total success every 100 episodes: [0]\n",
            "//////\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-ab059c2c6cfa>:86: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  action_apply = torch.nn.functional.softmax(action_apply)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total reward episode = -200.0\n",
            "Episode : 1\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 2\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 3\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 4\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 5\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 6\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 7\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 8\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 9\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 10\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 11\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 12\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 13\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 14\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 15\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 16\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 17\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 18\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 19\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 20\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 21\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 22\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 23\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 24\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 25\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 26\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 27\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 28\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 29\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 30\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 31\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 32\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 33\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 34\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 35\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 36\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 37\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 38\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 39\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 40\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 41\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 42\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 43\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 44\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 45\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 46\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 47\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 48\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 49\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 50\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 51\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 52\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 53\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 54\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 55\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 56\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 57\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 58\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 59\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 60\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 61\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 62\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 63\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 64\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 65\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 66\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 67\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 68\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 69\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 70\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 71\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 72\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 73\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 74\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 75\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 76\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 77\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 78\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 79\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 80\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 81\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 82\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 83\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 84\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 85\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 86\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 87\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 88\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 89\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 90\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 91\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 92\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 93\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 94\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 95\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 96\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 97\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 98\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 99\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 100\n",
            "//////\n",
            "Total success every 100 episodes: [0, 0]\n",
            "//////\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 101\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 102\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 103\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 104\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 105\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 106\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 107\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 108\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 109\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 110\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 111\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 112\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 113\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 114\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 115\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 116\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 117\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 118\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 119\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 120\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 121\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 122\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 123\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 124\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 125\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 126\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 127\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 128\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 129\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 130\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 131\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 132\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 133\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 134\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 135\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 136\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 137\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 138\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 139\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 140\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 141\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 142\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 143\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 144\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 145\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 146\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 147\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 148\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 149\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 150\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 151\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 152\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 153\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 154\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 155\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 156\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 157\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 158\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 159\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 160\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 161\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 162\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 163\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 164\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 165\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 166\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 167\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 168\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 169\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 170\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 171\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 172\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 173\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 174\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 175\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 176\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 177\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 178\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 179\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 180\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 181\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 182\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 183\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 184\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 185\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 186\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 187\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 188\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 189\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 190\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 191\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 192\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 193\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 194\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 195\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 196\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 197\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 198\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 199\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 200\n",
            "//////\n",
            "Total success every 100 episodes: [0, 0, 0]\n",
            "//////\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 201\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 202\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 203\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 204\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 205\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 206\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 207\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 208\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 209\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 210\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 211\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 212\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 213\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 214\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 215\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 216\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 217\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 218\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Episode : 219\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.9956], grad_fn=<AbsBackward>)\n",
            "Iter :-  0  Q_value Loss :-  0.5  Q_value Loss_2 :-  1.75  Policy Loss :-  0.45\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-e144b223de71>:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  new_action[i] = torch.nn.functional.softmax(new_action[i]) #+np.random.normal(0.0, 0.1))\n",
            "<ipython-input-18-e144b223de71>:79: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  new_action[i] = torch.nn.functional.softmax(new_action[i])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter :-  100  Q_value Loss :-  0.03  Q_value Loss_2 :-  0.33  Policy Loss :-  0.35\n",
            "Episode : 220\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -166.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  200  Q_value Loss :-  0.16  Q_value Loss_2 :-  0.8  Policy Loss :-  1.02\n",
            "Iter :-  300  Q_value Loss :-  0.21  Q_value Loss_2 :-  0.3  Policy Loss :-  1.14\n",
            "Episode : 221\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  1.24\n",
            "Iter :-  500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  1.63\n",
            "Episode : 222\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -173.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  1.74\n",
            "Iter :-  700  Q_value Loss :-  0.04  Q_value Loss_2 :-  0.07  Policy Loss :-  1.97\n",
            "Episode : 223\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.01  Policy Loss :-  2.22\n",
            "Iter :-  900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.01  Policy Loss :-  2.54\n",
            "Episode : 224\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -169.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.1153], grad_fn=<AbsBackward>)\n",
            "Iter :-  1000  Q_value Loss :-  0.01  Q_value Loss_2 :-  0.01  Policy Loss :-  2.76\n",
            "Iter :-  1100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  2.86\n",
            "Episode : 225\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -171.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  1200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  3.16\n",
            "Iter :-  1300  Q_value Loss :-  0.02  Q_value Loss_2 :-  0.02  Policy Loss :-  3.43\n",
            "Episode : 226\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  1400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.01  Policy Loss :-  3.67\n",
            "Iter :-  1500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  3.92\n",
            "Episode : 227\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  1600  Q_value Loss :-  0.01  Q_value Loss_2 :-  0.02  Policy Loss :-  4.13\n",
            "Iter :-  1700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  4.35\n",
            "Episode : 228\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Iter :-  1800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  4.6\n",
            "Iter :-  1900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  4.83\n",
            "Episode : 229\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -171.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0233], grad_fn=<AbsBackward>)\n",
            "Iter :-  2000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  5.06\n",
            "Iter :-  2100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  5.3\n",
            "Episode : 230\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -169.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  2200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  5.54\n",
            "Iter :-  2300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  5.75\n",
            "Episode : 231\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  2400  Q_value Loss :-  0.05  Q_value Loss_2 :-  0.02  Policy Loss :-  5.67\n",
            "Iter :-  2500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  6.14\n",
            "Episode : 232\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  2600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  6.41\n",
            "Iter :-  2700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  6.63\n",
            "Episode : 233\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -174.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  2800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  6.84\n",
            "Iter :-  2900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  7.07\n",
            "Episode : 234\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Value error = tensor([0.0372], grad_fn=<AbsBackward>)\n",
            "Iter :-  3000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  7.28\n",
            "Iter :-  3100  Q_value Loss :-  0.01  Q_value Loss_2 :-  0.01  Policy Loss :-  7.46\n",
            "Episode : 235\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Iter :-  3200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  7.7\n",
            "Iter :-  3300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  7.92\n",
            "Episode : 236\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  3400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  8.12\n",
            "Iter :-  3500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.01  Policy Loss :-  8.26\n",
            "Episode : 237\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  3600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  8.56\n",
            "Iter :-  3700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  8.74\n",
            "Episode : 238\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  3800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  9.01\n",
            "Iter :-  3900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  9.22\n",
            "Episode : 239\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -170.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0208], grad_fn=<AbsBackward>)\n",
            "Iter :-  4000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  9.43\n",
            "Iter :-  4100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  9.64\n",
            "Episode : 240\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -169.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  4200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  9.84\n",
            "Iter :-  4300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  10.08\n",
            "Episode : 241\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  4400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  10.32\n",
            "Iter :-  4500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  10.48\n",
            "Episode : 242\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -171.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  4600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  10.73\n",
            "Iter :-  4700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  10.95\n",
            "Episode : 243\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -170.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  4800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  11.13\n",
            "Iter :-  4900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  11.36\n",
            "Episode : 244\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -171.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0216], grad_fn=<AbsBackward>)\n",
            "Iter :-  5000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  11.58\n",
            "Iter :-  5100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  11.77\n",
            "Episode : 245\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -171.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  5200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  11.99\n",
            "Iter :-  5300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  12.2\n",
            "Episode : 246\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  5400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  12.37\n",
            "Iter :-  5500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  12.62\n",
            "Episode : 247\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -172.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  5600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  12.81\n",
            "Iter :-  5700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  13.01\n",
            "Episode : 248\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -169.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  5800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  13.23\n",
            "Iter :-  5900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  13.44\n",
            "Episode : 249\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0177], grad_fn=<AbsBackward>)\n",
            "Iter :-  6000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  13.65\n",
            "Iter :-  6100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  13.86\n",
            "Episode : 250\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  6200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  14.06\n",
            "Iter :-  6300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  14.28\n",
            "Episode : 251\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -171.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  6400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  14.48\n",
            "Iter :-  6500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  14.66\n",
            "Episode : 252\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -175.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  6600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  14.87\n",
            "Iter :-  6700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  15.04\n",
            "Episode : 253\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  6800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  15.23\n",
            "Iter :-  6900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  15.48\n",
            "Episode : 254\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0228], grad_fn=<AbsBackward>)\n",
            "Iter :-  7000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  15.7\n",
            "Iter :-  7100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  15.9\n",
            "Episode : 255\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  7200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  16.08\n",
            "Iter :-  7300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  16.32\n",
            "Episode : 256\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -188.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  7400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  16.51\n",
            "Iter :-  7500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  16.71\n",
            "Episode : 257\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  7600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  16.91\n",
            "Iter :-  7700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  17.1\n",
            "Episode : 258\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  7800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  17.31\n",
            "Iter :-  7900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  17.51\n",
            "Episode : 259\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -175.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0181], grad_fn=<AbsBackward>)\n",
            "Iter :-  8000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  17.69\n",
            "Iter :-  8100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  17.91\n",
            "Episode : 260\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  8200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  18.12\n",
            "Iter :-  8300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  18.29\n",
            "Episode : 261\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -166.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  8400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  18.5\n",
            "Iter :-  8500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  18.72\n",
            "Episode : 262\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -176.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  8600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  18.91\n",
            "Iter :-  8700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  19.08\n",
            "Episode : 263\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -171.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  8800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  19.32\n",
            "Iter :-  8900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  19.51\n",
            "Episode : 264\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0247], grad_fn=<AbsBackward>)\n",
            "Iter :-  9000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  19.72\n",
            "Iter :-  9100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  19.7\n",
            "Episode : 265\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Iter :-  9200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  20.1\n",
            "Iter :-  9300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  20.23\n",
            "Episode : 266\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  9400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  20.47\n",
            "Iter :-  9500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  20.68\n",
            "Episode : 267\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Iter :-  9600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  20.87\n",
            "Iter :-  9700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  21.06\n",
            "Episode : 268\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Iter :-  9800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  21.24\n",
            "Iter :-  9900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  21.4\n",
            "Episode : 269\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -172.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0181], grad_fn=<AbsBackward>)\n",
            "Iter :-  10000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  21.63\n",
            "Iter :-  10100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  21.83\n",
            "Episode : 270\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -169.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  10200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  22.02\n",
            "Iter :-  10300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  22.21\n",
            "Episode : 271\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  10400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  22.41\n",
            "Iter :-  10500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  22.6\n",
            "Episode : 272\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  10600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  22.79\n",
            "Iter :-  10700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  22.96\n",
            "Episode : 273\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -172.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  10800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  23.15\n",
            "Iter :-  10900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  23.35\n",
            "Episode : 274\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -172.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0027], grad_fn=<AbsBackward>)\n",
            "Iter :-  11000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  23.54\n",
            "Iter :-  11100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  23.72\n",
            "Episode : 275\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -165.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  11200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  23.91\n",
            "Iter :-  11300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  24.0\n",
            "Episode : 276\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  11400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  24.22\n",
            "Iter :-  11500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  24.44\n",
            "Episode : 277\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -176.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  11600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  24.66\n",
            "Iter :-  11700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  24.84\n",
            "Episode : 278\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -179.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  11800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  25.01\n",
            "Iter :-  11900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  25.21\n",
            "Episode : 279\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -173.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0010], grad_fn=<AbsBackward>)\n",
            "Iter :-  12000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  25.4\n",
            "Iter :-  12100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  25.57\n",
            "Episode : 280\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -170.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  12200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  25.74\n",
            "Iter :-  12300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  25.94\n",
            "Episode : 281\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  12400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  26.13\n",
            "Iter :-  12500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  26.32\n",
            "Episode : 282\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -171.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  12600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  26.42\n",
            "Iter :-  12700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  26.66\n",
            "Episode : 283\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -169.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  12800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  26.84\n",
            "Iter :-  12900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  27.03\n",
            "Episode : 284\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0131], grad_fn=<AbsBackward>)\n",
            "Iter :-  13000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  27.2\n",
            "Iter :-  13100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  27.38\n",
            "Episode : 285\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -169.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  13200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  27.61\n",
            "Iter :-  13300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  27.7\n",
            "Episode : 286\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  13400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  27.92\n",
            "Iter :-  13500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  28.09\n",
            "Episode : 287\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  13600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  28.27\n",
            "Iter :-  13700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  28.4\n",
            "Episode : 288\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  13800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  28.7\n",
            "Iter :-  13900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  28.6\n",
            "Episode : 289\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0309], grad_fn=<AbsBackward>)\n",
            "Iter :-  14000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  28.96\n",
            "Iter :-  14100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  29.14\n",
            "Episode : 290\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -169.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  14200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  29.32\n",
            "Iter :-  14300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  29.47\n",
            "Episode : 291\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -169.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  14400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  29.67\n",
            "Iter :-  14500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  29.84\n",
            "Episode : 292\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  14600  Q_value Loss :-  0.01  Q_value Loss_2 :-  0.0  Policy Loss :-  29.23\n",
            "Iter :-  14700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  30.2\n",
            "Episode : 293\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  14800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  30.37\n",
            "Iter :-  14900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  30.54\n",
            "Episode : 294\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0008], grad_fn=<AbsBackward>)\n",
            "Iter :-  15000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  30.71\n",
            "Iter :-  15100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  30.89\n",
            "Episode : 295\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  15200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  30.92\n",
            "Iter :-  15300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  31.22\n",
            "Episode : 296\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -169.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  15400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  31.4\n",
            "Iter :-  15500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  31.55\n",
            "Episode : 297\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -173.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  15600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  31.72\n",
            "Iter :-  15700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  31.88\n",
            "Episode : 298\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -172.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  15800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  32.06\n",
            "Iter :-  15900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  32.24\n",
            "Episode : 299\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -166.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.1063], grad_fn=<AbsBackward>)\n",
            "Iter :-  16000  Q_value Loss :-  0.01  Q_value Loss_2 :-  0.01  Policy Loss :-  32.31\n",
            "Iter :-  16100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  32.56\n",
            "Episode : 300\n",
            "//////\n",
            "Total success every 100 episodes: [0, 0, 0, 75]\n",
            "//////\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -169.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  16200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  32.72\n",
            "Iter :-  16300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  32.85\n",
            "Episode : 301\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -174.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  16400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  33.05\n",
            "Iter :-  16500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  33.21\n",
            "Episode : 302\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -170.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  16600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  33.37\n",
            "Iter :-  16700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  33.59\n",
            "Episode : 303\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  16800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  33.7\n",
            "Iter :-  16900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  33.88\n",
            "Episode : 304\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0090], grad_fn=<AbsBackward>)\n",
            "Iter :-  17000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  34.02\n",
            "Iter :-  17100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  34.13\n",
            "Episode : 305\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  17200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  34.35\n",
            "Iter :-  17300  Q_value Loss :-  0.01  Q_value Loss_2 :-  0.01  Policy Loss :-  34.39\n",
            "Episode : 306\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -176.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  17400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  34.67\n",
            "Iter :-  17500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  34.85\n",
            "Episode : 307\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Iter :-  17600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  34.92\n",
            "Iter :-  17700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  35.16\n",
            "Episode : 308\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  17800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  35.32\n",
            "Iter :-  17900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  35.4\n",
            "Episode : 309\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0695], grad_fn=<AbsBackward>)\n",
            "Iter :-  18000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  35.59\n",
            "Iter :-  18100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  35.81\n",
            "Episode : 310\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -175.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  18200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  35.96\n",
            "Iter :-  18300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  36.12\n",
            "Episode : 311\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -166.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  18400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  36.27\n",
            "Iter :-  18500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  36.49\n",
            "Episode : 312\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  18600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  36.58\n",
            "Iter :-  18700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  36.68\n",
            "Episode : 313\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -169.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  18800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  36.89\n",
            "Iter :-  18900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  37.06\n",
            "Episode : 314\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Value error = tensor([0.0246], grad_fn=<AbsBackward>)\n",
            "Iter :-  19000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  37.21\n",
            "Iter :-  19100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  37.36\n",
            "Episode : 315\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -176.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  19200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  37.52\n",
            "Iter :-  19300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  37.64\n",
            "Episode : 316\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  19400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  37.82\n",
            "Iter :-  19500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  37.98\n",
            "Episode : 317\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -173.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  19600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  38.14\n",
            "Iter :-  19700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  38.28\n",
            "Episode : 318\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  19800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  38.43\n",
            "Iter :-  19900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  38.63\n",
            "Episode : 319\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Value error = tensor([0.0069], grad_fn=<AbsBackward>)\n",
            "Iter :-  20000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  38.62\n",
            "Iter :-  20100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  38.83\n",
            "Episode : 320\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  20200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  39.03\n",
            "Iter :-  20300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  39.2\n",
            "Episode : 321\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -175.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  20400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  39.35\n",
            "Iter :-  20500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  39.5\n",
            "Episode : 322\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Iter :-  20600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  39.68\n",
            "Iter :-  20700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  39.78\n",
            "Episode : 323\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  20800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  39.96\n",
            "Iter :-  20900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  40.08\n",
            "Episode : 324\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -170.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0178], grad_fn=<AbsBackward>)\n",
            "Iter :-  21000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  40.22\n",
            "Iter :-  21100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  40.34\n",
            "Episode : 325\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  21200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  40.53\n",
            "Iter :-  21300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  40.68\n",
            "Episode : 326\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -166.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  21400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  40.85\n",
            "Iter :-  21500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  40.96\n",
            "Episode : 327\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -166.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  21600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  41.1\n",
            "Iter :-  21700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  41.27\n",
            "Episode : 328\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -180.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  21800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  41.39\n",
            "Iter :-  21900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  41.53\n",
            "Episode : 329\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -172.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0027], grad_fn=<AbsBackward>)\n",
            "Iter :-  22000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  41.67\n",
            "Iter :-  22100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  41.84\n",
            "Episode : 330\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  22200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  41.98\n",
            "Iter :-  22300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  42.12\n",
            "Episode : 331\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  22400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  42.18\n",
            "Iter :-  22500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  42.4\n",
            "Episode : 332\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -170.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  22600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  42.56\n",
            "Iter :-  22700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  42.69\n",
            "Episode : 333\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -171.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  22800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  42.8\n",
            "Iter :-  22900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  42.97\n",
            "Episode : 334\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -171.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0036], grad_fn=<AbsBackward>)\n",
            "Iter :-  23000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  43.11\n",
            "Iter :-  23100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  43.25\n",
            "Episode : 335\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -177.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  23200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  43.34\n",
            "Iter :-  23300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  43.57\n",
            "Episode : 336\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  23400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  43.6\n",
            "Iter :-  23500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  43.8\n",
            "Episode : 337\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -175.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  23600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  43.96\n",
            "Iter :-  23700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  44.08\n",
            "Episode : 338\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  23800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  44.2\n",
            "Iter :-  23900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  44.36\n",
            "Episode : 339\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Value error = tensor([0.0007], grad_fn=<AbsBackward>)\n",
            "Iter :-  24000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  44.51\n",
            "Iter :-  24100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  44.63\n",
            "Episode : 340\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -170.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  24200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  44.78\n",
            "Iter :-  24300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  44.91\n",
            "Episode : 341\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  24400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  45.03\n",
            "Iter :-  24500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  45.16\n",
            "Episode : 342\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -170.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  24600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  45.34\n",
            "Iter :-  24700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  45.45\n",
            "Episode : 343\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -169.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  24800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.01  Policy Loss :-  45.4\n",
            "Iter :-  24900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  45.73\n",
            "Episode : 344\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -175.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0104], grad_fn=<AbsBackward>)\n",
            "Iter :-  25000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  45.89\n",
            "Iter :-  25100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  45.93\n",
            "Episode : 345\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  25200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  46.13\n",
            "Iter :-  25300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  46.28\n",
            "Episode : 346\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  25400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  46.37\n",
            "Iter :-  25500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  46.58\n",
            "Episode : 347\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  25600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  46.71\n",
            "Iter :-  25700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  46.77\n",
            "Episode : 348\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -165.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  25800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  46.93\n",
            "Iter :-  25900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  47.09\n",
            "Episode : 349\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -169.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0162], grad_fn=<AbsBackward>)\n",
            "Iter :-  26000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  47.22\n",
            "Iter :-  26100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  47.29\n",
            "Episode : 350\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  26200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  47.45\n",
            "Iter :-  26300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  47.58\n",
            "Episode : 351\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  26400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  47.73\n",
            "Iter :-  26500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  47.85\n",
            "Episode : 352\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  26600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  47.97\n",
            "Iter :-  26700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  48.0\n",
            "Episode : 353\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  26800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  48.25\n",
            "Iter :-  26900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  48.36\n",
            "Episode : 354\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -169.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0039], grad_fn=<AbsBackward>)\n",
            "Iter :-  27000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  48.46\n",
            "Iter :-  27100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  48.5\n",
            "Episode : 355\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  27200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  48.73\n",
            "Iter :-  27300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  48.79\n",
            "Episode : 356\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -169.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  27400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  49.01\n",
            "Iter :-  27500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  49.13\n",
            "Episode : 357\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -174.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  27600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  49.25\n",
            "Iter :-  27700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  49.37\n",
            "Episode : 358\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -175.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  27800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  49.51\n",
            "Iter :-  27900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  49.65\n",
            "Episode : 359\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0205], grad_fn=<AbsBackward>)\n",
            "Iter :-  28000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.01  Policy Loss :-  49.57\n",
            "Iter :-  28100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  49.87\n",
            "Episode : 360\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -170.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  28200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  50.0\n",
            "Iter :-  28300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  50.05\n",
            "Episode : 361\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  28400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  50.26\n",
            "Iter :-  28500  Q_value Loss :-  0.01  Q_value Loss_2 :-  0.0  Policy Loss :-  50.08\n",
            "Episode : 362\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -170.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  28600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  50.49\n",
            "Iter :-  28700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  50.62\n",
            "Episode : 363\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -171.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  28800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  50.74\n",
            "Iter :-  28900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  50.88\n",
            "Episode : 364\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0643], grad_fn=<AbsBackward>)\n",
            "Iter :-  29000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  50.59\n",
            "Iter :-  29100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  51.11\n",
            "Episode : 365\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  29200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  51.27\n",
            "Iter :-  29300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  51.36\n",
            "Episode : 366\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  29400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  51.5\n",
            "Iter :-  29500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  51.47\n",
            "Episode : 367\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -170.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  29600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  51.71\n",
            "Iter :-  29700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  51.85\n",
            "Episode : 368\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -171.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  29800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  51.95\n",
            "Iter :-  29900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  52.07\n",
            "Episode : 369\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -175.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0090], grad_fn=<AbsBackward>)\n",
            "Iter :-  30000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  52.18\n",
            "Iter :-  30100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  52.29\n",
            "Episode : 370\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -166.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  30200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  52.42\n",
            "Iter :-  30300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  52.54\n",
            "Episode : 371\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -174.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  30400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  52.66\n",
            "Iter :-  30500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  52.79\n",
            "Episode : 372\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  30600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  52.89\n",
            "Iter :-  30700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  53.01\n",
            "Episode : 373\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  30800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  53.1\n",
            "Iter :-  30900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  53.26\n",
            "Episode : 374\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0144], grad_fn=<AbsBackward>)\n",
            "Iter :-  31000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  53.36\n",
            "Iter :-  31100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  53.48\n",
            "Episode : 375\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -172.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  31200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  53.59\n",
            "Iter :-  31300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  53.69\n",
            "Episode : 376\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -169.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  31400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  53.8\n",
            "Iter :-  31500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  53.95\n",
            "Episode : 377\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -171.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  31600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  54.02\n",
            "Iter :-  31700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  54.15\n",
            "Episode : 378\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -166.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  31800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  54.23\n",
            "Iter :-  31900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  54.38\n",
            "Episode : 379\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -170.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0020], grad_fn=<AbsBackward>)\n",
            "Iter :-  32000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  54.49\n",
            "Iter :-  32100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  54.63\n",
            "Episode : 380\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -170.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  32200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  54.76\n",
            "Iter :-  32300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  54.86\n",
            "Episode : 381\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Iter :-  32400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  54.93\n",
            "Iter :-  32500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  55.04\n",
            "Episode : 382\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -166.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  32600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  55.22\n",
            "Iter :-  32700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  55.3\n",
            "Episode : 383\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  32800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  55.39\n",
            "Iter :-  32900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  55.5\n",
            "Episode : 384\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -173.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([4.9591e-05], grad_fn=<AbsBackward>)\n",
            "Iter :-  33000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  55.63\n",
            "Iter :-  33100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  55.76\n",
            "Episode : 385\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -169.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  33200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  55.86\n",
            "Iter :-  33300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  55.96\n",
            "Episode : 386\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -174.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  33400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  56.05\n",
            "Iter :-  33500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  56.22\n",
            "Episode : 387\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -166.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  33600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  56.3\n",
            "Iter :-  33700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  56.38\n",
            "Episode : 388\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  33800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  56.51\n",
            "Iter :-  33900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  56.46\n",
            "Episode : 389\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -181.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0197], grad_fn=<AbsBackward>)\n",
            "Iter :-  34000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  56.72\n",
            "Iter :-  34100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  56.84\n",
            "Episode : 390\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  34200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  56.86\n",
            "Iter :-  34300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  57.08\n",
            "Episode : 391\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -169.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  34400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  57.16\n",
            "Iter :-  34500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  56.97\n",
            "Episode : 392\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -172.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  34600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  57.38\n",
            "Iter :-  34700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  57.48\n",
            "Episode : 393\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -174.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  34800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  57.55\n",
            "Iter :-  34900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  57.68\n",
            "Episode : 394\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0133], grad_fn=<AbsBackward>)\n",
            "Iter :-  35000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  57.81\n",
            "Iter :-  35100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  57.89\n",
            "Episode : 395\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -170.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  35200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  58.02\n",
            "Iter :-  35300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  58.13\n",
            "Episode : 396\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  35400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  58.25\n",
            "Iter :-  35500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  58.32\n",
            "Episode : 397\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -170.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  35600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  58.45\n",
            "Iter :-  35700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  58.57\n",
            "Episode : 398\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Iter :-  35800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  58.63\n",
            "Iter :-  35900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  58.75\n",
            "Episode : 399\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -172.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0110], grad_fn=<AbsBackward>)\n",
            "Iter :-  36000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  58.89\n",
            "Iter :-  36100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  58.96\n",
            "Episode : 400\n",
            "//////\n",
            "Total success every 100 episodes: [0, 0, 0, 75, 93]\n",
            "//////\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  36200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  59.08\n",
            "Iter :-  36300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  59.11\n",
            "Episode : 401\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  36400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  59.3\n",
            "Iter :-  36500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  59.43\n",
            "Episode : 402\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  36600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  59.43\n",
            "Iter :-  36700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  59.56\n",
            "Episode : 403\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -166.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  36800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  59.69\n",
            "Iter :-  36900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  59.83\n",
            "Episode : 404\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -169.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0672], grad_fn=<AbsBackward>)\n",
            "Iter :-  37000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  59.9\n",
            "Iter :-  37100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  59.97\n",
            "Episode : 405\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Iter :-  37200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  60.1\n",
            "Iter :-  37300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  60.19\n",
            "Episode : 406\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  37400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  60.29\n",
            "Iter :-  37500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  60.37\n",
            "Episode : 407\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -171.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  37600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  60.47\n",
            "Iter :-  37700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  60.56\n",
            "Episode : 408\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  37800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  60.67\n",
            "Iter :-  37900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  60.78\n",
            "Episode : 409\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Value error = tensor([0.0260], grad_fn=<AbsBackward>)\n",
            "Iter :-  38000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  60.88\n",
            "Iter :-  38100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  61.02\n",
            "Episode : 410\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  38200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  61.12\n",
            "Iter :-  38300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  61.18\n",
            "Episode : 411\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -171.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  38400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  61.25\n",
            "Iter :-  38500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  61.32\n",
            "Episode : 412\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -169.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  38600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  61.49\n",
            "Iter :-  38700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  61.64\n",
            "Episode : 413\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -171.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  38800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  61.61\n",
            "Iter :-  38900  Q_value Loss :-  0.01  Q_value Loss_2 :-  0.0  Policy Loss :-  61.66\n",
            "Episode : 414\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -173.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0070], grad_fn=<AbsBackward>)\n",
            "Iter :-  39000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  61.85\n",
            "Iter :-  39100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.11  Policy Loss :-  54.15\n",
            "Episode : 415\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -169.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  39200  Q_value Loss :-  4.45  Q_value Loss_2 :-  1.75  Policy Loss :-  52.27\n",
            "Iter :-  39300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  62.09\n",
            "Episode : 416\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  39400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  62.27\n",
            "Iter :-  39500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  62.38\n",
            "Episode : 417\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -168.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  39600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  62.49\n",
            "Iter :-  39700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  62.54\n",
            "Episode : 418\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -172.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  39800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  62.63\n",
            "Iter :-  39900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  62.85\n",
            "Episode : 419\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0115], grad_fn=<AbsBackward>)\n",
            "Iter :-  40000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  62.84\n",
            "Iter :-  40100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.01  Policy Loss :-  62.97\n",
            "Episode : 420\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  40200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  63.07\n",
            "Iter :-  40300  Q_value Loss :-  0.01  Q_value Loss_2 :-  0.02  Policy Loss :-  59.97\n",
            "Episode : 421\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -166.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  40400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  63.21\n",
            "Iter :-  40500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  63.34\n",
            "Episode : 422\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -171.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  40600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  63.43\n",
            "Iter :-  40700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  63.56\n",
            "Episode : 423\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -169.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  40800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  63.66\n",
            "Iter :-  40900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  63.74\n",
            "Episode : 424\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Value error = tensor([0.0130], grad_fn=<AbsBackward>)\n",
            "Iter :-  41000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  63.79\n",
            "Iter :-  41100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  64.02\n",
            "Episode : 425\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -173.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  41200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  63.97\n",
            "Iter :-  41300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  64.04\n",
            "Episode : 426\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -177.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  41400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  64.18\n",
            "Iter :-  41500  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  64.25\n",
            "Episode : 427\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -167.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  41600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  64.38\n",
            "Iter :-  41700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  64.19\n",
            "Episode : 428\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -200.0\n",
            "Iter :-  41800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  64.51\n",
            "Iter :-  41900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  64.66\n",
            "Episode : 429\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -171.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0053], grad_fn=<AbsBackward>)\n",
            "Iter :-  42000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  64.71\n",
            "Iter :-  42100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  64.79\n",
            "Episode : 430\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -169.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  42200  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  64.9\n",
            "Iter :-  42300  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  65.0\n",
            "Episode : 431\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -172.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  42400  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  65.13\n",
            "Iter :-  42500  Q_value Loss :-  0.01  Q_value Loss_2 :-  0.0  Policy Loss :-  65.09\n",
            "Episode : 432\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -169.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  42600  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  65.38\n",
            "Iter :-  42700  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  65.34\n",
            "Episode : 433\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -173.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Iter :-  42800  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  65.42\n",
            "Iter :-  42900  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  65.52\n",
            "Episode : 434\n",
            "Step number = 0 Reward = -1.0\n",
            "Step number = 100 Reward = -1.0\n",
            "Total reward episode = -169.0\n",
            "////////\n",
            "Goal Reached !!\n",
            "////////\n",
            "Value error = tensor([0.0037], grad_fn=<AbsBackward>)\n",
            "Iter :-  43000  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  65.61\n",
            "Iter :-  43100  Q_value Loss :-  0.0  Q_value Loss_2 :-  0.0  Policy Loss :-  65.71\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-ab059c2c6cfa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m          \u001b[0msoft_q_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m          \u001b[0mupdate_ratio\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m          \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-e144b223de71>\u001b[0m in \u001b[0;36msoft_q_update\u001b[0;34m(batch_size, update_ratio, update, gamma, mean_lambda, std_lambda, z_lambda, soft_tau, total_q_value_loss, total_q_value_loss_2, total_value_loss, total_policy_loss, total_alpha_loss)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m      \u001b[0;32mfor\u001b[0m \u001b[0mtarget_param\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_q_net_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoft_q_net_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m        \u001b[0mtarget_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msoft_tau\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msoft_tau\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m    \u001b[0;31m#Store losses for plot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "replay_buffer = Bio_inspired_Buffer(replay_buffer_size)\n",
        "\n",
        "epsilon = 1.0\n",
        "\n",
        "nb_success = 0\n",
        "list_success = []\n",
        "\n",
        "for ep in range(3000):\n",
        "\n",
        "   state = [0.0] * 10\n",
        "\n",
        "   if cpt == True:\n",
        "\n",
        "      i = 0\n",
        "\n",
        "      while i < 200:\n",
        "\n",
        "         soft_q_update(1, update_ratio, update=True)\n",
        "         update_ratio += 1\n",
        "         i += 1\n",
        "   '''\n",
        "   if cpt == True:\n",
        "        \n",
        "      for target_param, param in zip(policy_net_perturbed.parameters(), policy_net.parameters()):\n",
        "          target_param.data.copy_(param.data)\n",
        "\n",
        "      with torch.no_grad():\n",
        "\n",
        "         policy_net_perturbed.noise(noise_std)\n",
        "         #soft_q_net.noise(noise_std)\n",
        "         #soft_q_net_2.noise(noise_std)\n",
        "      \n",
        "      noise_std = noise_std/1.005\n",
        "\n",
        "      print(\"Noise Std = \" + str(noise_std))\n",
        "   '''\n",
        "   obs = env.reset()\n",
        "   '''\n",
        "   for i in obs:\n",
        "      list_obs.pop(0)\n",
        "      list_obs.append(i)\n",
        "   obs = list_obs\n",
        "   print(\"Episode : \" +str(ep))\n",
        "   '''\n",
        "\n",
        "   print(\"Episode : \" +str(ep))\n",
        "\n",
        "   #obs = list_obs\n",
        "\n",
        "   reward_ep = 0\n",
        "\n",
        "   if cpt == False:\n",
        "\n",
        "      policy_net = Actor(2, 3).to(device)\n",
        "   \n",
        "   for i in obs:\n",
        "     state.pop(0)\n",
        "     state.append(i)\n",
        "  \n",
        "   if ep % 100 == 0:\n",
        "\n",
        "     list_success.append(nb_success)\n",
        "     nb_success = 0\n",
        "     print(\"//////\")\n",
        "     print(\"Total success every 100 episodes: \" + str(list_success))\n",
        "     print(\"//////\")\n",
        "\n",
        "   for step in range(num_steps):\n",
        "       # take random action, but you can also do something more intelligent\n",
        "       # action = my_intelligent_agent_fn(obs) \n",
        "\n",
        "       nb_step += 1\n",
        "             \n",
        "       #else:\n",
        "       \n",
        "       with torch.no_grad():\n",
        "\n",
        "          #if cpt == False:\n",
        "\n",
        "            #action_apply = policy_net_perturbed.forward(torch.FloatTensor(state).to(device)) #env.action_space.sample()\n",
        "\n",
        "          #else:\n",
        "\n",
        "          action_apply = policy_net.forward(torch.FloatTensor(state).to(device))\n",
        "\n",
        "          action_apply = torch.nn.functional.softmax(action_apply)\n",
        "\n",
        "          action_apply = torch.argmax(action_apply,dim=0).tolist()\n",
        "       \n",
        "       if np.random.uniform(0.0, 1.0) < 0.01 and cpt == True:\n",
        "         \n",
        "         action_apply = np.random.randint(3)\n",
        "       '''\n",
        "       if epsilon > 0.10:\n",
        "\n",
        "         epsilon = epsilon - 0.000001\n",
        "\n",
        "       else:\n",
        "\n",
        "         epsilon = 0.10\n",
        "\n",
        "       if nb_step % 10000 == 0:\n",
        "         print(\"/////\")\n",
        "         print(\"Epsilon  = \" + str(epsilon))\n",
        "         print(\"/////\")\n",
        "       '''\n",
        "       # apply the action\n",
        "       next_state, reward, done, info = env.step(action_apply)\n",
        "       '''\n",
        "       for i in next_state:\n",
        "         \n",
        "         list_obs.pop(0)\n",
        "         list_obs.append(i)\n",
        "       next_state = list_obs\n",
        "       '''\n",
        "       next_s = state\n",
        "       \n",
        "       for i in next_state:\n",
        "         next_s.pop(0)\n",
        "         next_s.append(i)\n",
        "       \n",
        "       replay_buffer.push(np.float32(state), float(action_apply), reward, np.float32(next_s), done, 0)\n",
        "       '''\n",
        "       for i in range(len(obs)):\n",
        "         \n",
        "         list_obs.pop(0)\n",
        "         list_obs.append(obs[i]- next_state[i])\n",
        "\n",
        "       for i in range(len(list_obs)):\n",
        "\n",
        "         list_obs[i] = list_obs[i] + np.random.normal(0.0, 0.01)\n",
        "       '''\n",
        "       \n",
        "       #obs = next_state\n",
        "       state = next_s\n",
        "\n",
        "       reward_ep += reward\n",
        "\n",
        "       # Render the env\n",
        "       #env.render()\n",
        "\n",
        "       # Wait a bit before the next frame unless you want to see a crazy fast video\n",
        "       time.sleep(0.001)\n",
        "       \n",
        "       if step % 100 == 0:\n",
        "\n",
        "         print(\"Step number = \" + str(step) + \" Reward = \" + str(reward))\n",
        "       \n",
        "       # If the epsiode is up, then start another one\n",
        "       if done:\n",
        "\n",
        "         print(\"Total reward episode = \" + str(reward_ep))\n",
        "\n",
        "         if reward_ep > -200:\n",
        "\n",
        "            nb_success += 1\n",
        "            cpt = True\n",
        "            print(\"////////\")\n",
        "            print(\"Goal Reached !!\")\n",
        "            print(\"////////\")\n",
        "         \n",
        "         #if cpt == False:\n",
        "\n",
        "           #replay_buffer = Bio_inspired_Buffer(replay_buffer_size)\n",
        "         \n",
        "         env.reset()\n",
        "\n",
        "   # Close the env\n",
        "   #env.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJbltNlQamwZHg98ENEdXq",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}